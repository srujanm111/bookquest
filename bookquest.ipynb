{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "import chromadb\n",
    "import os\n",
    "import textract\n",
    "import tiktoken\n",
    "import math\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "BOOK_FILE = \"Downloads/quantum.html\"\n",
    "CHUNK_SIZE = 400\n",
    "NUM_DOCUMENTS = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "QUERY = \"What are the philosophical implications of quantum mechanics?\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1) read the text from the file\n",
    "text = textract.process(BOOK_FILE).decode()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2) split the text into tokens\n",
    "encoding = tiktoken.encoding_for_model(GPT_MODEL)\n",
    "tokens = encoding.encode(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3) split the tokens into chunks\n",
    "num_chunks = math.ceil(len(tokens) / CHUNK_SIZE)\n",
    "token_chunks = [tokens[i*CHUNK_SIZE:min((i+1)*CHUNK_SIZE, len(tokens))] for i in range(0, num_chunks)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4) convert the token chunks into documents\n",
    "documents = [encoding.decode(chunk) for chunk in token_chunks]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5) transform the documents into embeddings\n",
    "embeddings = [openai.Embedding.create(input=doc, model=EMBEDDING_MODEL)[\"data\"][0][\"embedding\"] for doc in documents]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6) create a new document for each embedded text\n",
    "embedding_function = OpenAIEmbeddingFunction(openai.api_key, EMBEDDING_MODEL)\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name='collection', embedding_function=embedding_function)\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings,\n",
    "    ids=[str(i) for i in range(len(documents))],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7) convert the query into embedding\n",
    "query_embedding = openai.Embedding.create(input=QUERY, model=EMBEDDING_MODEL)[\"data\"][0][\"embedding\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 8) search the collection for the most similar embeddings\n",
    "query_result = collection.query(query_embeddings=query_embedding, n_results=NUM_DOCUMENTS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 9) return the most similar documents in text form\n",
    "most_similar_documents = query_result[\"documents\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 10) give ChatGPT the most similar documents and the query\n",
    "documents_context = \"\\n\".join(most_similar_documents)\n",
    "system_message = f'''\n",
    "You have been given pieces of text from a larger file named {BOOK_FILE}.\n",
    "You will use the following information from the contents of the file to answer the user's question. The information is contained within triple backticks.\n",
    "```{documents_context}```\n",
    "'''\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"Here is my question: {QUERY}\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "answer = response.choices[0][\"message\"][\"content\"]\n",
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
